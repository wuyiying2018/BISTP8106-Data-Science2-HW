---
title: "Homework 1" 
author: "Yiying Wu (yw3996)"
output:
  pdf_document:
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[L]{Data Science 2 HW1}
- \fancyhead[R]{Yiying Wu (yw3996)}
- \fancypagestyle{plain}{\pagestyle{fancy}}
--- 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## R packages
```{r}
library(tidyverse)
library(caret)
library(tidymodels)
```

## Input dataset
```{r}
housing_train<-read_csv("./data/housing_training.csv")
housing_train <- na.omit(housing_train)
housing_test<-read_csv("./data/housing_test.csv")
housing_test <- na.omit(housing_test)
```

**Response: Sale price**

## (a) Fit a lasso model on the training data. Report the selected tuning parameter and the test error. When the 1SE rule is applied, how many predictors are included in the model?

```{r}
ctrl1 <- trainControl(method = "repeatedcv", 
                      number = 10,
                      repeats = 5,
                      selectionFunction = "oneSE")

# Lasso 
set.seed(8106)
lasso.fit <- train(Sale_Price ~ .,
                   data = housing_train,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(10, 0, length = 200))),
                   trControl = ctrl1)
# plot(lasso.fit, xTrans = log)
```
Here's the selected tuning parameter when 1SE rule is applied
```{r}
lasso.fit$bestTune
```
The best tuning parameter is `r round(lasso.fit$bestTune$lambda,3)`

And the test error is
```{r}
lasso.pred <- predict(lasso.fit, newdata = housing_test)
# test error
mean((lasso.pred - housing_test$Sale_Price)^2)
```
MSE=`r mean((lasso.pred - housing_test$Sale_Price)^2)`

coefficients in the final model are
```{r}
# coefficients in the final model
coef(lasso.fit$finalModel, lasso.fit$bestTune$lambda)
```

Therefore, there are 29 predictors included in the model.

## (b) Fit an elastic net model on the training data. Report the selected tuning parameters and the test error. Is it possible to apply the 1SE rule to select the tuning parameters for elastic net? If the 1SE rule is applicable, implement it to select the tuning parameters. If not, explain why.

Using the minimal MSE rule
```{r}
ctrl2 <- trainControl(method = "repeatedcv", 
                      number = 10,
                      repeats = 5,
                      selectionFunction = "best")

set.seed(8106)
enet.fit2 <- train(Sale_Price ~ .,
                   data = housing_train,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(10, 0, length = 200))),
                   trControl = ctrl2)
```
Here's the selected tuning parameter
```{r}
enet.fit2$bestTune
```
The best tuning parameter is `r round(enet.fit2$bestTune$lambda,3)`

And the test error is
```{r}
enet.pred2 <- predict(enet.fit2, newdata = housing_test)
# test error
mean((enet.pred2 - housing_test$Sale_Price)^2)
```
MSE=`r mean((enet.pred2 - housing_test$Sale_Price)^2)`

Using the 1SE rule
```{r}
set.seed(8106)
enet.fit1 <- train(Sale_Price ~ .,
                   data = housing_train,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(10, 0, length = 200))),
                   trControl = ctrl1)
```
Here's the selected tuning parameter
```{r}
enet.fit1$bestTune
```
The best tuning parameter is `r round(enet.fit1$bestTune$lambda,3)`

And the test error is
```{r}
enet.pred1 <- predict(enet.fit1, newdata = housing_test)
# test error
mean((enet.pred1 - housing_test$Sale_Price)^2)
```
MSE=`r mean((enet.pred1 - housing_test$Sale_Price)^2)`

Given the substantial difference in lambda values between the minimal MSE and the 1SE rule in the results, it suggests that the simpler model under the 1SE rule is significantly more regularized. Given that the 1SE rule led to a model with lower MSE on the test data, it would be reasonable to favor this approach for selecting tuning parameters in the elastic net model.

Also, the change from alpha = 0.05 to alpha = 0 under the 1SE rule indicates a shift from a slight Lasso preference towards a pure Ridge regression approach. In this way, all predictors are kept in the model, leading to models that may be less sparse but can handle multicollinearity better. 

## (c) Fit a partial least squares model on the training data and report the test error. How many components are included in your model?

```{r}

```


## (d) Choose the best model for predicting the response and explain your choice.

## (e) If “caret” was used for the elastic net in (b), retrain this model with “tidymodels”, and vice versa. Compare the selected tuning parameters between the two software approaches. Should there be discrepancies in the chosen parameters, discuss potential reasons for these differences.