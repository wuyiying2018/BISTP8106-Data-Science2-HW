---
title: "Homework 1" 
author: "Yiying Wu (yw3996)"
output:
  pdf_document:
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[L]{Data Science 2 HW1}
- \fancyhead[R]{Yiying Wu (yw3996)}
- \fancypagestyle{plain}{\pagestyle{fancy}}
--- 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## R packages
```{r}
library(tidyverse)
library(caret)
library(tidymodels)
```

## Input dataset
```{r}
housing_train<-read_csv("./data/housing_training.csv")
housing_train <- na.omit(housing_train)
housing_test<-read_csv("./data/housing_test.csv")
housing_test <- na.omit(housing_test)
```

**Response: Sale price**

## (a) Fit a lasso model on the training data. Report the selected tuning parameter and the test error. When the 1SE rule is applied, how many predictors are included in the model?

```{r}
ctrl1 <- trainControl(method = "repeatedcv", 
                      number = 10,
                      repeats = 5,
                      selectionFunction = "oneSE")

ctrl2 <- trainControl(method = "repeatedcv", 
                      number = 10,
                      repeats = 5,
                      selectionFunction = "best")
```

Using the minimal MSE rule
```{r}
set.seed(123)
lasso.fit2 <- train(Sale_Price ~ .,
                    data = housing_train,
                    method = "glmnet",
                    tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(10, 0, length = 200))),
                    trControl = ctrl2)
# plot(lasso.fit1, xTrans = log)
```

Here's the selected tuning parameter when the minimal MSE rule is applied
```{r}
lasso.fit2$bestTune
```

The best tuning parameter is `r round(lasso.fit2$bestTune$lambda,3)`

And the test error is
```{r}
lasso.pred2 <- predict(lasso.fit2, newdata = housing_test)
# test error
mean((lasso.pred2 - housing_test$Sale_Price)^2)
```
MSE=`r mean((lasso.pred2 - housing_test$Sale_Price)^2)`

Using 1SE rule
```{r}
set.seed(123)
lasso.fit1 <- train(Sale_Price ~ .,
                    data = housing_train,
                    method = "glmnet",
                    tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(10, 0, length = 200))),
                    trControl = ctrl1)
# plot(lasso.fit1, xTrans = log)
```

Here's the selected tuning parameter when 1SE rule is applied
```{r}
lasso.fit1$bestTune
```
The best tuning parameter is `r round(lasso.fit1$bestTune$lambda,3)`

And the test error is
```{r}
lasso.pred1 <- predict(lasso.fit1, newdata = housing_test)
# test error
mean((lasso.pred1 - housing_test$Sale_Price)^2)
```
MSE=`r mean((lasso.pred1 - housing_test$Sale_Price)^2)`

coefficients in the final model are
```{r}
# coefficients in the final model
coef(lasso.fit1$finalModel, lasso.fit1$bestTune$lambda)
```

Therefore, there are 36 predictors included in the model.

## (b) Fit an elastic net model on the training data. Report the selected tuning parameters and the test error. Is it possible to apply the 1SE rule to select the tuning parameters for elastic net? If the 1SE rule is applicable, implement it to select the tuning parameters. If not, explain why.

Using the minimal MSE rule
```{r}
set.seed(123)
enet.fit2 <- train(Sale_Price ~ .,
                   data = housing_train,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(10, 0, length = 200))),
                   trControl = ctrl2)
```
Here's the selected tuning parameter
```{r}
enet.fit2$bestTune
```
The best tuning parameter is `r round(enet.fit2$bestTune$lambda,3)`

And the test error is
```{r}
enet.pred2 <- predict(enet.fit2, newdata = housing_test)
# test error
mean((enet.pred2 - housing_test$Sale_Price)^2)
```
MSE=`r mean((enet.pred2 - housing_test$Sale_Price)^2)`

Using the 1SE rule
```{r}
set.seed(123)
enet.fit1 <- train(Sale_Price ~ .,
                   data = housing_train,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(10, 0, length = 200))),
                   trControl = ctrl1)
```
Here's the selected tuning parameter
```{r}
enet.fit1$bestTune
```
The best tuning parameter is `r round(enet.fit1$bestTune$lambda,3)`

And the test error is
```{r}
enet.pred1 <- predict(enet.fit1, newdata = housing_test)
# test error
mean((enet.pred1 - housing_test$Sale_Price)^2)
```
MSE=`r mean((enet.pred1 - housing_test$Sale_Price)^2)`

Given the substantial difference in lambda values between the minimal MSE and the 1SE rule in the results, it suggests that the simpler model under the 1SE rule is significantly more regularized. Given that the 1SE rule led to a model with lower MSE on the test data, it would be reasonable to favor this approach for selecting tuning parameters in the elastic net model.

Also, the change from alpha = 0.05 to alpha = 0 under the 1SE rule indicates a shift from a slight Lasso preference towards a pure Ridge regression approach. In this way, all predictors are kept in the model, leading to models that may be less sparse but can handle multicollinearity better. 

## (c) Fit a partial least squares model on the training data and report the test error. How many components are included in your model?

```{r}
# training data
x <- model.matrix(Sale_Price ~ ., housing_train)[, -1]
y <- housing_train$Sale_Price

# test data
x2 <- model.matrix(Sale_Price ~ .,housing_test)[, -1]
y2 <- housing_test$Sale_Price

set.seed(123)
pls.fit <- train(x, y,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:19),
                 trControl = ctrl2,
                 preProcess = c("center", "scale"))

```

the test error is
```{r}
predy2.pls2 <- predict(pls.fit, newdata = x2)
mean((y2 - predy2.pls2)^2)
```
MSE=`r mean((y2 - predy2.pls2)^2)`

Check the number of components included in the model
```{r}
ggplot(pls.fit, highlight = TRUE)
```

8 components are included in the model.

## (d) Choose the best model for predicting the response and explain your choice.

```{r}
resamp <- resamples(list(elastic_net = enet.fit2, 
                         elastic_net_1se = enet.fit1, 
                         lasso = lasso.fit2, 
                         lasso_1se = lasso.fit1, 
                         pls = pls.fit))
summary(resamp)

bwplot(resamp, metric = "RMSE")
```

The best model for predicting the sale price of a house is the elastic net model since it has the lowest mean value of RMSE comparing to all other models.

## (e) If “caret” was used for the elastic net in (b), retrain this model with “tidymodels”, and vice versa. Compare the selected tuning parameters between the two software approaches. Should there be discrepancies in the chosen parameters, discuss potential reasons for these differences.

```{r}
set.seed(123)
cv_folds <- vfold_cv(housing_train, v = 10) 

enet_spec <- linear_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet") %>% 
  set_mode("regression")

# enet_spec %>% extract_parameter_dials("mixture")
# enet_spec %>% extract_parameter_dials("penalty")

enet_grid_set <- parameters(penalty(range = c(2, 10), trans = log_trans()),
                            mixture(range = c(0, 1)))
enet_grid <- grid_regular(enet_grid_set, levels = c(80, 20))



enet_workflow <- workflow() %>%
  add_model(enet_spec) %>%
  add_formula(Sale_Price ~ .)

enet_tune <- tune_grid(
  enet_workflow,
  resamples = cv_folds,
  grid = enet_grid
)

autoplot(enet_tune, metric = "rmse") + 
  theme(legend.position = "top") +
  labs(color = "Mixing Percentage\n(Alpha Values)") 

enet_best <- select_best(enet_tune, metric = "rmse") 

final_enet_spec <- enet_spec %>% 
  update(penalty = enet_best$penalty, mixture = enet_best$mixture)

enet_fit <- fit(final_enet_spec, formula = Sale_Price ~ ., data = housing_train)

# Get coefficients
enet_model <- extract_fit_engine(enet_fit)
coef(enet_model, s = enet_best$penalty)

```
select tuning parameters using tidymodels package
```{r}
enet_best$penalty
```
The selected tuning parameters is `r round(enet_best$penalty,3)`, which is different from that is part **(b)**. This maybe because different partitions are used in tidymodels and caret, which likely contributes to discrepancies in the chosen parameters for elastic net models between the two frameworks.
